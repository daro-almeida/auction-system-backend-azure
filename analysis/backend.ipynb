{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Types\n",
    "\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    min: float\n",
    "    max: float\n",
    "    count: float\n",
    "    p50: float\n",
    "    median: float\n",
    "    p75: float\n",
    "    p90: float\n",
    "    p95: float\n",
    "    p99: float\n",
    "    p999: float\n",
    "\n",
    "\n",
    "class Metrics(BaseModel):\n",
    "    first_counter_at: pd.Timestamp = Field(alias=\"firstCounterAt\")\n",
    "    last_counter_at: pd.Timestamp = Field(alias=\"lastCounterAt\")\n",
    "    counters: dict[str, int]\n",
    "    rates: dict[str, int]\n",
    "    summaries: dict[str, Summary]\n",
    "\n",
    "    @validator(\"first_counter_at\", \"last_counter_at\")\n",
    "    def validate_first_counter_at(cls, v):\n",
    "        return pd.Timestamp(v)\n",
    "\n",
    "\n",
    "class Report(BaseModel):\n",
    "    region: str\n",
    "    georeplicated: bool\n",
    "    cache_enabled: bool\n",
    "\n",
    "    aggregate: Metrics\n",
    "    intermediate: list[Metrics]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "SUMMARY_HTTP_RESPONSE_TIME = \"http.response_time\"\n",
    "\n",
    "SUMMARY_TYPE_READ = \"read\"\n",
    "SUMMARY_TYPE_WRITE = \"write\"\n",
    "SUMMARY_TYPE_READ_BLOB = \"read blob\"\n",
    "SUMMARY_TYPE_WRITE_BLOB = \"write blob\"\n",
    "SUMMARY_TYPE_USER_AUTH = \"user auth\"\n",
    "SUMMARY_TYPE_USER_CREATE = \"user create\"\n",
    "SUMMARY_TYPE_USER_AUCTIONS = \"user auctions\"\n",
    "SUMMARY_TYPE_USER_FOLLOWING = \"user following\"\n",
    "SUMMARY_TYPE_AUCTIONS_POPULAR = \"auctions popular\"\n",
    "SUMMARY_TYPE_AUCTIONS_RECENT = \"auctions recent\"\n",
    "SUMMARY_TYPE_AUCTIONS_CREATE = \"auction create\"\n",
    "SUMMARY_TYPE_BID_CREATE = \"bid create\"\n",
    "SUMMARY_TYPE_AUCTION_BIDS = \"auction bids\"\n",
    "SUMMARY_TYPE_AUCTION_QUESTIONS = \"auction questions\"\n",
    "SUMMARY_TYPE_QUESTION_CREATE = \"question create\"\n",
    "SUMMARY_TYPE_REPLY_CREATE = \"reply create\"\n",
    "SUMMARY_ENDPOINTS = {\n",
    "    # User Endpoints\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/user/auth\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_USER_AUTH,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/user\": [\n",
    "        SUMMARY_TYPE_WRITE,\n",
    "        SUMMARY_TYPE_USER_CREATE,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/user/*/auctions\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_USER_AUCTIONS,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/user/*/following\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_USER_FOLLOWING,\n",
    "    ],\n",
    "    # Auction Endpoints\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/auction/any/popular\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_AUCTIONS_POPULAR,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/auction/any/recent\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_AUCTIONS_RECENT,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/auction/*/bid\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_AUCTION_BIDS,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/auction/*/question\": [\n",
    "        SUMMARY_TYPE_READ,\n",
    "        SUMMARY_TYPE_AUCTION_QUESTIONS,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/auction\": [\n",
    "        SUMMARY_TYPE_WRITE,\n",
    "        SUMMARY_TYPE_AUCTIONS_CREATE,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/auction/*/bid\": [\n",
    "        SUMMARY_TYPE_WRITE,\n",
    "        SUMMARY_TYPE_BID_CREATE,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/auction/*/question\": [\n",
    "        SUMMARY_TYPE_WRITE,\n",
    "        SUMMARY_TYPE_QUESTION_CREATE,\n",
    "    ],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/auction/*/question/*/reply\": [\n",
    "        SUMMARY_TYPE_WRITE,\n",
    "        SUMMARY_TYPE_REPLY_CREATE,\n",
    "    ],\n",
    "    # Media Endpoints\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/media\": [SUMMARY_TYPE_READ_BLOB],\n",
    "    \"plugins.metrics-by-endpoint.response_time.GET:/media/*\": [SUMMARY_TYPE_READ_BLOB],\n",
    "    \"plugins.metrics-by-endpoint.response_time.POST:/media\": [SUMMARY_TYPE_WRITE_BLOB],\n",
    "}\n",
    "SUMMARY_IGNORED = [\"vusers.session_length\"]\n",
    "\n",
    "\n",
    "def parse_report_dir(name: str) -> tuple[str, str, str]:\n",
    "    geo, cache, region = name.split(\"_\")\n",
    "    geo = geo.split(\"=\")[1] == \"1\"\n",
    "    cache = cache.split(\"=\")[1] == \"1\"\n",
    "    region = region.split(\"=\")[1]\n",
    "    return geo, cache, region\n",
    "\n",
    "\n",
    "def read_reports(geo: bool, cache: bool, region: str) -> dict[str, Report]:\n",
    "    geos = \"1\" if geo else \"0\"\n",
    "    caches = \"1\" if cache else \"0\"\n",
    "    dir = f\"reports/geo={geos}_cache={caches}_region={region}\"\n",
    "    reports = {}\n",
    "    for file in os.listdir(dir):\n",
    "        with open(f\"{dir}/{file}\") as f:\n",
    "            obj = json.load(f)\n",
    "            aggregate = Metrics.parse_obj(obj[\"aggregate\"])\n",
    "            intermediate = [Metrics.parse_obj(i) for i in obj[\"intermediate\"]]\n",
    "            reports[file.removesuffix(\".json\")] = Report(\n",
    "                region=region,\n",
    "                georeplicated=geo,\n",
    "                cache_enabled=cache,\n",
    "                aggregate=aggregate,\n",
    "                intermediate=intermediate,\n",
    "            )\n",
    "    return reports\n",
    "\n",
    "\n",
    "def read_reports2(region: str) -> dict[str, Report]:\n",
    "    reports = {}\n",
    "    with open(f\"reports/{region}.json\") as f:\n",
    "        obj = json.load(f)\n",
    "        aggregate = Metrics.parse_obj(obj[\"aggregate\"])\n",
    "        intermediate = [Metrics.parse_obj(i) for i in obj[\"intermediate\"]]\n",
    "        reports[\"workload1\"] = Report(\n",
    "            region=region,\n",
    "            georeplicated=False,\n",
    "            cache_enabled=False,\n",
    "            aggregate=aggregate,\n",
    "            intermediate=intermediate,\n",
    "        )\n",
    "    return reports\n",
    "\n",
    "\n",
    "def summary_to_series(summary: Summary) -> pd.Series:\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"min\": summary.min,\n",
    "            \"max\": summary.max,\n",
    "            \"count\": summary.count,\n",
    "            \"median\": summary.median,\n",
    "            \"p75\": summary.p75,\n",
    "            \"p90\": summary.p90,\n",
    "            \"p95\": summary.p95,\n",
    "            \"p99\": summary.p99,\n",
    "            \"p999\": summary.p999,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def reports_http_means(reports: Iterable[Report]) -> pd.Series:\n",
    "    series = [\n",
    "        summary_to_series(s.aggregate.summaries[SUMMARY_HTTP_RESPONSE_TIME])\n",
    "        for s in reports\n",
    "    ]\n",
    "    return pd.concat(series, axis=1).mean(axis=1)\n",
    "\n",
    "\n",
    "def reports_summaries_by_type(reports: Iterable[Report]) -> pd.DataFrame:\n",
    "    def count_for_name(counters: dict[str, int], name: str) -> int:\n",
    "        counter_partial_name = name.replace(\".response_time\", \"\")\n",
    "        acum = 0\n",
    "        for counter_name, counter_value in counters.items():\n",
    "            if counter_partial_name in counter_name and (\n",
    "                \"200\" or \"204\" in counter_name\n",
    "            ):\n",
    "                acum += counter_value\n",
    "        return acum\n",
    "\n",
    "    summaries_per_type = defaultdict(list)\n",
    "    weights_per_type = defaultdict(list)\n",
    "    summary_per_type = {}\n",
    "\n",
    "    for report in reports:\n",
    "        counters = report.aggregate.counters\n",
    "        for name, summary in report.aggregate.summaries.items():\n",
    "            if name == SUMMARY_HTTP_RESPONSE_TIME or name in SUMMARY_IGNORED:\n",
    "                continue\n",
    "            if name not in SUMMARY_ENDPOINTS:\n",
    "                raise ValueError(f\"Unknown summary: {name}\")\n",
    "            summary_types = SUMMARY_ENDPOINTS[name]\n",
    "            summary_count = count_for_name(counters, name)\n",
    "            for summary_type in summary_types:\n",
    "                summaries_per_type[summary_type].append(summary_to_series(summary))\n",
    "                weights_per_type[summary_type].append(summary_count)\n",
    "\n",
    "    for summary_type, summaries in summaries_per_type.items():\n",
    "        weights = weights_per_type[summary_type]\n",
    "        scaled = [s * w for s, w in zip(summaries, weights)]\n",
    "        summary_per_type[summary_type] = pd.concat(scaled, axis=1).sum(axis=1) / sum(\n",
    "            weights\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(summary_per_type)\n",
    "\n",
    "\n",
    "def reports_summaries_medians_by_type(reports: Iterable[Report]) -> pd.Series:\n",
    "    return (\n",
    "        reports_summaries_by_type(reports)\n",
    "        .drop(index=[\"min\", \"max\"])\n",
    "        .transpose()[\"median\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def report_summaries_by_type(report: Report) -> pd.DataFrame:\n",
    "    return reports_summaries_by_type([report])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = {\"workload1\": read_reports(False, False, \"westeurope\")[\"workload1\"]}\n",
    "r2 = {\"workload1\": read_reports(False, True, \"westeurope\")[\"workload1\"]}\n",
    "r3 = read_reports2(\"europe\")\n",
    "r4 = read_reports2(\"us\")\n",
    "r5 = read_reports2(\"local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = reports_summaries_medians_by_type(r1.values())\n",
    "c2 = reports_summaries_medians_by_type(r2.values())\n",
    "c3 = reports_summaries_medians_by_type(r3.values())\n",
    "c4 = reports_summaries_medians_by_type(r4.values())\n",
    "c5 = reports_summaries_medians_by_type(r5.values())\n",
    "\n",
    "\n",
    "def prep_data(d: pd.Series) -> pd.Series:\n",
    "    return d\n",
    "\n",
    "\n",
    "latencies = pd.DataFrame(\n",
    "    {\n",
    "        \"West Europe Cache Disabled\": prep_data(c1),\n",
    "        \"West Europe Cache Enabled\": prep_data(c2),\n",
    "        \"West Europe Kubernetes\": prep_data(c3),\n",
    "        \"Central US Kubernetes\": prep_data(c4),\n",
    "        \"Margem Sul\": prep_data(c5),\n",
    "    },\n",
    ").transpose()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "labels = latencies.index.values\n",
    "x = np.arange(len(labels))\n",
    "colors = [sns.color_palette()[i] for i in range(len(labels))]\n",
    "yread = latencies[\"read\"].values\n",
    "ywrite = latencies[\"write\"].values\n",
    "\n",
    "axs[0].bar(x, yread, label=labels, color=colors)\n",
    "axs[0].legend(title=\"Configuration\")\n",
    "axs[0].get_xaxis().set_visible(False)\n",
    "axs[0].set_ylabel(\"Average Latency (ms)\")\n",
    "axs[0].set_title(\"Read Latencies\")\n",
    "\n",
    "axs[1].bar(x, ywrite, label=labels, color=colors)\n",
    "axs[1].legend(title=\"Configuration\")\n",
    "axs[1].get_xaxis().set_visible(False)\n",
    "axs[1].set_ylabel(\"Average Latency (ms)\")\n",
    "axs[1].set_title(\"Write Latencies\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"latencies.pdf\", facecolor=\"white\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies.rename(\n",
    "    {\n",
    "        \"West Europe Cache Disabled\": \"WECOFF\",\n",
    "        \"West Europe Cache Enabled\": \"WECON\",\n",
    "        \"West Europe Kubernetes\": \"WEUK8S\",\n",
    "        \"Central US Kubernetes\": \"CUSAK8S\",\n",
    "        \"Margem Sul\": \"MS\",\n",
    "    },\n",
    ").loc[[\"WEUK8S\", \"CUSAK8S\"]][\n",
    "    [\n",
    "        SUMMARY_TYPE_USER_AUTH,\n",
    "        SUMMARY_TYPE_USER_AUCTIONS,\n",
    "        SUMMARY_TYPE_USER_FOLLOWING,\n",
    "        SUMMARY_TYPE_AUCTIONS_POPULAR,\n",
    "        SUMMARY_TYPE_AUCTIONS_RECENT,\n",
    "        SUMMARY_TYPE_AUCTION_BIDS,\n",
    "        SUMMARY_TYPE_AUCTION_QUESTIONS,\n",
    "    ]\n",
    "].plot(\n",
    "    kind=\"bar\", rot=0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
